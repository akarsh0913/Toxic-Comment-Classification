import numpy as np
import pandas as pd
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

df = pd.read_csv('train.csv')
df = df.drop('id', axis=1)

df['comment_text'] = df['comment_text'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))
df['comment_text'] = df['comment_text'].apply(lambda x: x.lower())
stop_words = set(stopwords.words('english'))
df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))
lemmatizer = WordNetLemmatizer()
df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))

comments_with_label_1 = ((df['toxic']==1) | (df['severe_toxic']==1) | (df['obscene']==1)
                        | (df['threat']==1) | (df['insult']==1) | (df['identity_hate']==1)).sum()

print(f'Total number of comments with value 1 in any label: {comments_with_label_1}')

comments_with_all_labels_1 = ((df['toxic']==1) & (df['severe_toxic']==1) & (df['obscene']==1)
                        & (df['threat']==1) & (df['insult']==1) & (df['identity_hate']==1)).sum()

print(f'Total number of comments with value 1 in all labels: {comments_with_all_labels_1}')

num_non_toxic_comments = ((df['toxic'] == 0) & (df['severe_toxic'] == 0) &
                          (df['obscene'] == 0) & (df['threat'] == 0) &
                          (df['insult'] == 0) & (df['identity_hate'] == 0)).sum()

print(f'Total number of non-toxic comments: {num_non_toxic_comments}')

non_toxic_comments = df[df['toxic'] == 0]
non_toxic_sample = non_toxic_comments.sample(n=15600, random_state=42)
toxic_comments = df[df['toxic'] == 1]
new_df = pd.concat([non_toxic_sample, toxic_comments], axis=0)
new_df = new_df.reset_index(drop=True)

num_non_toxic_comments = ((new_df['toxic'] == 0) & (new_df['severe_toxic'] == 0) &
                          (new_df['obscene'] == 0) & (new_df['threat'] == 0) &
                          (new_df['insult'] == 0) & (new_df['identity_hate'] == 0)).sum()

print(f'Total number of non-toxic comments: {num_non_toxic_comments}')

comments_with_label_1 = ((new_df['toxic']==1) | (new_df['severe_toxic']==1) | (new_df['obscene']==1)
                        | (new_df['threat']==1) | (new_df['insult']==1) | (new_df['identity_hate']==1)).sum()

print(f'Total number of comments with value 1 in any label: {comments_with_label_1}')

new_df.to_csv('new_train.csv', index=False)
