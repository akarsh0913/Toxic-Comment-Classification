import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
from sklearn.svm import SVC
import tensorflow
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, MaxPooling1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

df = pd.read_csv('new_train.csv')
df.isna().sum()
df = df.dropna()

labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
label_counts = df[labels].sum()
plt.bar(labels, label_counts)
plt.title('Class Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

toxic_lengths = df[df['toxic'] == 1]['comment_text'].apply(lambda x: len(x.split()))
severe_toxic_lengths = df[df['severe_toxic'] == 1]['comment_text'].apply(lambda x: len(x.split()))
obscene_lengths = df[df['obscene'] == 1]['comment_text'].apply(lambda x: len(x.split()))
threat_lengths = df[df['threat'] == 1]['comment_text'].apply(lambda x: len(x.split()))
insult_lengths = df[df['insult'] == 1]['comment_text'].apply(lambda x: len(x.split()))
identity_hate_lengths = df[df['identity_hate'] == 1]['comment_text'].apply(lambda x: len(x.split()))
plt.hist(toxic_lengths, bins=50, alpha=0.5, label='toxic')
plt.hist(severe_toxic_lengths, bins=50, alpha=0.5, label='severe_toxic')
plt.hist(obscene_lengths, bins=50, alpha=0.5, label='obscene')
plt.hist(threat_lengths, bins=50, alpha=0.5, label='threat')
plt.hist(insult_lengths, bins=50, alpha=0.5, label='insult')
plt.hist(identity_hate_lengths, bins=50, alpha=0.5, label='identity_hate')
plt.legend(loc='upper right')
plt.title('Comment Length Distribution by Label')
plt.xlabel('Comment Length')
plt.ylabel('Frequency')
plt.show()

vectorizer = CountVectorizer()
text_data = vectorizer.fit_transform(df['comment_text'])
word_counts = text_data.sum(axis=0)
word_freq = [(word, word_counts[0, i]) for word, i in vectorizer.vocabulary_.items()]
word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)[:20]
plt.bar(range(len(word_freq)), [val[1] for val in word_freq])
plt.xticks(range(len(word_freq)), [val[0] for val in word_freq], rotation=45, ha='right')
plt.title('Most Common Words')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.show()

sia = SentimentIntensityAnalyzer()
toxic_comments = df[df['toxic'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in toxic_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for toxic")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

severe_toxic_comments = df[df['severe_toxic'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in severe_toxic_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for severe_toxic")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

obscene_comments = df[df['obscene'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in obscene_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for obscene")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

threat_comments = df[df['threat'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in threat_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for threat")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

insult_comments = df[df['insult'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in insult_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for insult")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

identity_hate_comments = df[df['identity_hate'] == 1]['comment_text']
pos_sentiments = []
neg_sentiments = []
for comment in identity_hate_comments:
    sentences = sent_tokenize(comment)
    for sentence in sentences:
        sentiment = sia.polarity_scores(sentence)
        pos_sentiments.append(sentiment['pos'])
        neg_sentiments.append(sentiment['neg'])
print("Sentiment Analysis for identity_hate")
print("Positive sentiment: ", np.mean(pos_sentiments))
print("Negative sentiment: ", np.mean(neg_sentiments))

stop_words = set(stopwords.words('english'))
toxic_comments = df[df['toxic'] == 1]['comment_text'].tolist()
toxic_words = []
for comment in toxic_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            toxic_words.append(word.lower())
toxic_word_freq = nltk.FreqDist(toxic_words)
toxic_word_df = pd.DataFrame({'Word': list(toxic_word_freq.keys()), 'Frequency': list(toxic_word_freq.values())})
toxic_word_df = toxic_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=toxic_word_df.head(30))
plt.title('Most Common Bad Words in Toxic Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

severe_toxic_comments = df[df['severe_toxic'] == 1]['comment_text'].tolist()
severe_toxic_words = []
for comment in severe_toxic_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            severe_toxic_words.append(word.lower())
severe_toxic_word_freq = nltk.FreqDist(severe_toxic_words)
severe_toxic_word_df = pd.DataFrame({'Word': list(severe_toxic_word_freq.keys()), 'Frequency': list(severe_toxic_word_freq.values())})
severe_toxic_word_df = severe_toxic_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=severe_toxic_word_df.head(30))
plt.title('Most Common Bad Words in Severe Toxic Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

obscene_comments = df[df['obscene'] == 1]['comment_text'].tolist()
obscene_words = []
for comment in obscene_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            obscene_words.append(word.lower())
obscene_word_freq = nltk.FreqDist(obscene_words)
obscene_word_df = pd.DataFrame({'Word': list(obscene_word_freq.keys()), 'Frequency': list(obscene_word_freq.values())})
obscene_word_df = obscene_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=obscene_word_df.head(30))
plt.title('Most Common Bad Words in Obscene Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

threat_comments = df[df['threat'] == 1]['comment_text'].tolist()
threat_words = []
for comment in threat_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            threat_words.append(word.lower())
threat_word_freq = nltk.FreqDist(threat_words)
threat_word_df = pd.DataFrame({'Word': list(threat_word_freq.keys()), 'Frequency': list(threat_word_freq.values())})
threat_word_df = threat_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=threat_word_df.head(30))
plt.title('Most Common Bad Words in Threat Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

insult_comments = df[df['insult'] == 1]['comment_text'].tolist()
insult_words = []
for comment in insult_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            insult_words.append(word.lower())
insult_word_freq = nltk.FreqDist(insult_words)
insult_word_df = pd.DataFrame({'Word': list(insult_word_freq.keys()), 'Frequency': list(insult_word_freq.values())})
insult_word_df = insult_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=insult_word_df.head(30))
plt.title('Most Common Bad Words in Insult Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

identity_hate_comments = df[df['identity_hate'] == 1]['comment_text'].tolist()
identity_hate_words = []
for comment in identity_hate_comments:
    words = comment.split()
    for word in words:
        if word.isalpha() and word not in stop_words:
            identity_hate_words.append(word.lower())
identity_hate_word_freq = nltk.FreqDist(identity_hate_words)
identity_hate_word_df = pd.DataFrame({'Word': list(identity_hate_word_freq.keys()), 'Frequency': list(identity_hate_word_freq.values())})
identity_hate_word_df = identity_hate_word_df.sort_values(by='Frequency', ascending=False)
sns.set_style("whitegrid")
plt.figure(figsize=(8,8))
sns.barplot(x='Word', y='Frequency', data=identity_hate_word_df.head(30))
plt.title('Most Common Bad Words in Identity Hate Comments')
plt.xlabel('Bad Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

X = text_data
y = np.array(df.iloc[:, 1:])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape:', X_train.shape)
print('y_train shape:', y_train.shape)
print('X_test shape:', X_test.shape)
print('y_test shape:', y_test.shape)

scaler = StandardScaler(with_mean=False)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

for i in range(6):
    label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'][i]
    y_train_label = y_train[:, i]
    y_test_label = y_test[:, i]
    mnb = MultinomialNB(alpha=0.1)
    mnb.fit(X_train, y_train_label)
    train_acc = mnb.score(X_train, y_train_label)
    test_acc = mnb.score(X_test, y_test_label)
    print(f"Label: {label}")
    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")
    y_train_pred = mnb.predict(X_train)
    y_test_pred = mnb.predict(X_test)
    print(f"\nTrain Classification Report for label {label}:")
    print(classification_report(y_train_label, y_train_pred))
    print(f"Test Classification Report for label {label}:")
    print(classification_report(y_test_label, y_test_pred))
    print("\n")

for i in range(6):
    label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'][i]
    y_train_label = y_train[:, i]
    y_test_label = y_test[:, i]
    svm = SVC(C=1, kernel='linear', gamma='scale', decision_function_shape='ovr')
    svm.fit(X_train, y_train_label)
    train_acc = svm.score(X_train, y_train_label)
    test_acc = svm.score(X_test, y_test_label)
    print(f"Label: {label}")
    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")
    y_train_pred = svm.predict(X_train)
    y_test_pred = svm.predict(X_test)
    print(f"\nTrain Classification Report for label {label}:")
    print(classification_report(y_train_label, y_train_pred))
    print(f"Test Classification Report for label {label}:")
    print(classification_report(y_test_label, y_test_pred))
    print("\n")

lstm_cnn_model = Sequential()
lstm_cnn_model.add(Embedding(10000, 128, input_length=200))
lstm_cnn_model.add(Conv1D(128, 5, activation='relu'))
lstm_cnn_model.add(MaxPooling1D(5))
lstm_cnn_model.add(Conv1D(128, 5, activation='relu'))
lstm_cnn_model.add(MaxPooling1D(5))
lstm_cnn_model.add(LSTM(128))
lstm_cnn_model.add(Dense(128, activation='relu'))
lstm_cnn_model.add(Dropout(0.5))
lstm_cnn_model.add(Dense(6, activation='sigmoid'))
lstm_cnn_model.summary()

lstm_cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

lstm_cnn_model.fit(train_data, train_labels, epochs=10, batch_size=128, validation_data=(val_data, val_labels))

target_names = ['toxic', 'severe toxic', 'obscene', 'threat', 'insult', 'identity hate']
lstm_cnn_train_pred = lstm_cnn_model.predict(train_data)
lstm_cnn_train_pred = (lstm_cnn_train_pred > 0.5).astype(int)
print(classification_report(train_labels, lstm_cnn_train_pred, target_names=target_names))

lstm_cnn_val_pred = lstm_cnn_model.predict(val_data)
lstm_cnn_val_pred = (lstm_cnn_val_pred > 0.5).astype(int)
print(classification_report(val_labels, lstm_cnn_val_pred, target_names=target_names))
